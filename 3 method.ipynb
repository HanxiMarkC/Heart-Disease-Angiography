{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "49ecb266-7f0e-4973-aeae-77c5ea62ae7b",
   "metadata": {},
   "source": [
    "### Methods and Plan"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "963a3f9d-33ce-4f9c-9c40-4fcb0d38b57f",
   "metadata": {},
   "source": [
    "#### Train-test split:\n",
    "\n",
    "To assess the performance and generalization of the model, it will be trained on the training set and then evaluated on the testing set to estimate its performance on new, unseen data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fce05767-e13d-43c3-8b69-b95ba5ffcad4",
   "metadata": {},
   "source": [
    "#### LASSO with logistic regression:\n",
    "\n",
    "Since we have binary response, where 0 means patient without heart disease and 1 means the patient is diagnosed with heart disease, I choose LASSO regression as the predictive model due to its ability to handle binary outcomes, perform variable selection, and prevent overfitting, and I specify the family parameter as \"binomial\" to indicate logistic regression and alpha = 1.\n",
    "\n",
    "LASSO incorporates a penalty term that imposes a constraint on the absolute values of the regression coefficients by effectively setting some coefficients to exactly zero. In the context of our research question, this is good because it helps identify the most influential variables among location, age, sex, chest pain type, resting blood pressure, serum cholesterol, and resting ECG results that contribute to the likelihood of coronary disease. In addition, its shrinkage method regularizes the regression coefficients by penalizing large values. This is particularly beneficial when dealing with datasets that may have multicollinearity or a large number of predictors. The shrinkage of coefficients helps prevent overfitting and increases the model's ability to generalize to new, unseen data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fde8b7e7-c595-4ba0-9b14-8ce2085630f9",
   "metadata": {},
   "source": [
    "#### Assumptions: \n",
    "\n",
    "Logistic Regression makes several assumptions:\n",
    "\n",
    "- Linearity: The relationship between the log-odds of the dependent variable and the independent variables should be linear.\n",
    "- Independence of Observations: The observations should be independent of each other.\n",
    "- Absence of Multicollinearity: Logistic Regression assumes that there is little or no multicollinearity among the independent variables. This can be checked by variance inflation factor (VIF)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1461d6b6-daef-48df-979b-fcd3c5fb4a67",
   "metadata": {},
   "source": [
    "#### Hyperparameter Tuning\n",
    "\n",
    "I will create a list of possible lambdas and pass it to cv.glmnet() will be used to find an \"optimal\" value of lambda by cross-validation, which creates many test sets from the training set. We chose the one with least MSE.\n",
    "In addition, I will use plot() to check and visualize the estimated regression coefficients over the lambda grid."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29221eb2-865c-498c-8703-20f88f176c59",
   "metadata": {},
   "source": [
    "#### Model Evaluation on Testing Set\n",
    "Assess the performance of the trained model on the testing set. Maybe I will compare the prediction accuracy of LASSO with my group members who did other method like LS in the same test set(by setting the same seed)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64036d5a-1d61-4938-8abc-3939ed4aeafd",
   "metadata": {},
   "source": [
    "#### Limitations:\n",
    "- Model Complexity: While LASSO helps with variable selection, it also increases bias if the true model is complex.\n",
    "- Sensitive to Outliers: Logistic Regression is sensitive to outliers, which may impact our model performance. And it is sensitive to the scale of the predictor variables. I will fit 2 models, one with standardized predictors and one without on the same train set and evaluate on test set to compare the model performance.\n",
    "- Assumption of Linearity: Logistic Regression assumes a linear relationship between the log-odds of the dependent variable and the independent variables. If this assumption is strongly violated, the model may not perform well."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "4.2.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
